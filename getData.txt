## let's start by trying to get some of Richard Cronn's needle
## data from NCBI/SRA

## following examples from:
https://github.com/ncbi/sra-tools/wiki/Download-On-Demand

## many but not all runs listed at:
https://www.ncbi.nlm.nih.gov/Traces/study/?acc=SRP018395

## just starting with one:

## SRA has its own set of command line tools. 

prefetch SRR669789

## while we're at it....
prefetch SRR669871  
prefetch SRR669834  

## where are these going? 

srapath SRR669834  
"/home/daniel/ncbi/public/sra/"

## these are not 

## how do we get all of the runs or samples associated
## with a project?

tar -xzf edirect.tar.gz

#################

## okay, but what if need genbank gene acessions, not the 
## read libraries?

esearch -db protein -query "lycopene cyclase" | efetch -format fasta

esearch -db nucleotide -query "lycopene cyclase [prot] fungi [ORGN]" | efetch -format full -mode json |& tee "lycopene.json"


esearch -db  -query "lignin peroxidase [nt] fungi [ORGN] NOT bacteria [ORGN] " | efetch -format full -mode json

esearch -db nucleotide -query "lignin peroxidase" -spell -field DEFINITION

## plan: 

## check size of Cronn project, see if we have room on this computer to download all. 
##  if not, just grab a couple more to train methods
## get mRNA sequences for them all:

cutinase[All Fields] AND (fungi[filter] AND biomol_mrna[PROP])


## how can we do this with entrez?

## figure out how to query keywords and limit to fungi.

## get list of terms of interest. 

## can we take top, best hits?


## ugh. Another night and nothing to show for it. 

## Okay, we need to show the case for two ideas:

## 1 - there are fungal reads in these studies. 

## 2 - there are correlated, potentially ecologically meaningful 
##     changes in the fungal-origin and host RNA. 

## so - for #1, get . Set up the runinfo, do the following:

## download a sample
## blast it for the set of fungal indicators
## collect counts and run metadata
## delete r

## let's tool around in talapas for a bit:

## get an interactive session in talapas

srun --pty bash

## our storage is at:

cd /projects/xylaria/dthomas

## modules
module load blast
module load bbmap

## we need to get a list of all the projects/samples
## of Dougfir from SRA:

## there are three projects I'm interested in from the Cronn folks:
PRJNA263611
PRJNA188506
PRJNA243096
## and one project from the EPA:
PRJNA421903


## to get a list of runs/samples from this project

proj='PRJNA188506' 
wget 'http://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?save=efetch&rettype=runinfo&db=sra&term='$proj -O SraRunInfo2.csv

## okay, so once you have this, how do you get the files?

## I think we just want the first column:

bb=$(cut -d ',' -f 1  SraRunInfo.csv)

for i in ${bb}; do 
echo $i
done

## taking just the last one, can we download the sample using this value?

## can we get sra tools work locally on talapas?
wget https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-centos_linux64.tar.gz
ln -s /projects/xylaria/dthomas/sratoolkit/bin/prefetch /projects/xylaria/dthomas/prefetch
ln -s /projects/xylaria/dthomas/sratoolkit/bin/fastq-dump /projects/xylaria/dthomas/fastq-dump

## anyway, can get the raw reads from one SRA run ID?

./fastq-dump -Z $i > $i.fastq

## seems to be working...

## got to convert to fasta - best utility for this?
## I remember bbmap was good...

reformat.sh in=$i.fastq out=$i.fasta

## now to blast it for sequences of interest?

makeblastdb -in $i.fasta -parse_seqids -dbtype nucl &



## after getting the results, cleanup:
## rm $i.fastq 

## accidentally put the fastq at:
ls -l /projects/xylaria/dthomas/sratoolkit/bin/SRR669871.fastq

## we need some genes:

## how to build a fasta with what we need?


## ergosterol:

## we want everything after lanosterol for the 
## ergosterol pathway, I guess. Our genbank
## search terms:

## Ergosterol[Title] AND (fungi[filter] AND biomol_mrna[PROP])




## fungal chitan synthase

## this is my search:
## chitin synthase[Title] AND (fungi[filter] AND biomol_mrna[PROP])
## then I kept the top 195 records

## from home comp:
scp chitin_synthase_search_Genbank.fasta dthomas@talapas-ln1.uoregon.edu:/projects/xylaria/dthomas 

scp Ergosterol_search_Genbank.fasta dthomas@talapas-ln1.uoregon.edu:/projects/xylaria/dthomas 

## do a search with these?

blastn -db $i.fasta -query chitinase_search_Genbank.fasta -out $i.chs.out

## nada. is it because the queries are so long? Make a small one:

blastn -db $i.fasta -query test.fasta -out test.chs.out

## nope. Hmm. Well, this may in fact be a bad idea...

## anyway, let's take this to it's logical end. Tonight:
## 1 - compile full fasta of genes to look for, ergosterols and chitin synthases
## 2 - figure out a good blast output format to score hits, if any, quickly
## 3 - write script to loop through one project
## 4 - apply this to all 4 projects

## and talapas is down... fuck. This has been a difficult grant application
## to write...

## new plan - start on manuscript edits. Maybe the text for the grant

## hit the ground running on this weekend. By then I should have access to 
## either wm or uo cluster or both. 

## but I need to know that I'm not crazy here, that this is isn't a waste 
## of time. 

## let's use the lab computer till talapas is back online. 

## two projects - the above involves checking the entire read set for the 
## presence of a set of fungal-specific reads. The other half of my 
## preliminary data idea was to simply get an idea of how many reads are 
## thrown out from an RNAseq study wsimply due to not aligning to the 
## ref genome or transcriptome. 

## these are complimentary. One iteration of the loop should look this:

## download a run or sample
## align it 
## discard reads that align
## note amount of non-aligned reads
## blast the remaining sample with the selected sequences from ergosterol and chitin synthase

## get the Dougfir genome:

cd /home/daniel/Documents/genomes/Pseudotsuga

wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/001/517/045/GCA_001517045.1_DougFir1.0/GCA_001517045.1_DougFir1.0_genomic.fna.gz

tar -xzf GCA_001517045.1_DougFir1.0_genomic.fna.gz

## that's going to take forever. But what are we going to do with it when it's done?

## make an index file for  hisat:
hisatDir=/home/daniel/hisat2-2.1.0

$hisatDir/hisat2-build GCA_001517045.1_DougFir1.0_genomic.fna dougFirGen

##########################
#!usr/bin/env bash
hisat2-build GCA_001517045.1_DougFir1.0_genomic.fna dougFirGen

## get list of runs for the first project

proj='PRJNA188506' 
wget 'http://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?save=efetch&rettype=runinfo&db=sra&term='$proj -O SraRunInfo.csv

## make an array of run names:

bb=$(cut -d ',' -f 1  <(sed '1,1 d' SraRunInfo.csv ))

## get the fastq

for i in $bb; do
echo $i
    ## get fastq
    ./fastq-dump -Z $i > $i.fastq
    ## do the alignment
    sa=${i/\.fastq/\.sam}
    $hisatDir/hisat2 -x mllGen -U $i -S $sa 
    ## convert to BAM
    ba=${i/\.sam/\.bam}
    samtools view -bS $sa > $ba
    ## or convert to fasta?
    clean up 
    rm $sa
    rm $i
    ## we need to count lines here
done


$hisatDir/hisat2

## 

reformat.sh in=$i.fastq out=$i.fasta
makeblastdb -in $i.fasta -parse_seqids -dbtype nucl &


## setup scp for dougfir genome:

scp GCA_001517045.1_DougFir1.0_genomic.fna.gz dthomas@talapas-login.uoregon.edu:/projects/xylaria/dthomas

## get hisat

wget ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/downloads/hisat2-2.1.0-Linux_x86_64.zip

## can we get a job going to build the hisat index?

#!/bin/bash
#SBATCH --partition=short        ### Partition (like a queue in PBS)
#SBATCH --job-name=build_dougfir_index
#SBATCH --output=build.log         ### File in which to store job output
#SBATCH --error=build.err          ### File in which to store job error messages
#SBATCH --time=0-04:01:00       ### Wall clock time limit in Days-HH:MM:SS
#SBATCH --nodes=1               ### Number of nodes needed for the job
#SBATCH --ntasks-per-node=1     ### Number of tasks to be launched per Node

hisat2-2.1.0/hisat2-build GCA_001517045.1_DougFir1.0_genomic.fna dougfir

scp hisatbuild.sh dthomas@talapas-login.uoregon.edu:/projects/xylaria/dthomas

## we're going to do two large loops, (1) through all the samples in a project, 
## for (2) all projects 

## so let's run through one sample from one project:


project='PRJNA188506' 
wget 'http://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?save=efetch&rettype=runinfo&db=sra&term='$project -O $project".csv"

module load samtools


for i in ${bb[@]}; do
echo $i
done

######### 
#!/bin/bash
#SBATCH --partition=short        ### Partition (like a queue in PBS)
#SBATCH --job-name=build_dougfir_index
#SBATCH --output=build.log         ### File in which to store job output
#SBATCH --error=build.err          ### File in which to store job error messages
#SBATCH --time=0-23:00:00       ### Wall clock time limit in Days-HH:MM:SS
#SBATCH --nodes=1               ### Number of nodes needed for the job
#SBATCH --ntasks-per-node=1     ### Number of tasks to be launched per Node
#SBATCH --memory=64G

module load samtools
module load bbmap
cd /projects/xylaria/dthomas

project='PRJNA188506' 
wget 'http://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?save=efetch&rettype=runinfo&db=sra&term='$project -O $project".csv"
bb=($(cut -d ',' -f 1  <(sed '1,1 d' $project".csv" )))

for i in ${bb[@]}; do
    echo $i
    ./fastq-dump -Z $i > $i.fastq
    hisat2-2.1.0/hisat2 -x dougfir -U $i.fastq -S $i.sam 
    rm $i.fastq
    samtools view -f 4 -b $i.sam > $i"_notMapped.bam"
    rm $i.sam
    ## save this for later, keeps failing:
    #reformat.sh in=$i"_notMapped.bam" out=$i"_notMapped.fastq"
done

## cleanup:
#find . -type f -newer notMatch.sh -exec "rm" {} \;

############################################################

aa=('PRJNA263611' 'PRJNA188506' 'PRJNA243096' 'PRJNA421903')
for j in ${aa[@]}; do
echo $j
done

## now what? 

## we should take our fastq file of unmatched reads, and blast 
## them against known fungal genes. 

## which genes?




