## let's start by trying to get some of Richard Cronn's needle
## data from NCBI/SRA

## following examples from:
https://github.com/ncbi/sra-tools/wiki/Download-On-Demand

## many but not all runs listed at:
https://www.ncbi.nlm.nih.gov/Traces/study/?acc=SRP018395

## just starting with one:

## SRA has its own set of command line tools. 

prefetch SRR669789

## while we're at it....
prefetch SRR669871  
prefetch SRR669834  

## where are these going? 

srapath SRR669834  
"/home/daniel/ncbi/public/sra/"

## these are not 

## how do we get all of the runs or samples associated
## with a project?

tar -xzf edirect.tar.gz

#################

## okay, but what if need genbank gene acessions, not the 
## read libraries?

esearch -db protein -query "lycopene cyclase" | efetch -format fasta

esearch -db nucleotide -query "lycopene cyclase [prot] fungi [ORGN]" | efetch -format full -mode json |& tee "lycopene.json"


esearch -db  -query "lignin peroxidase [nt] fungi [ORGN] NOT bacteria [ORGN] " | efetch -format full -mode json

esearch -db nucleotide -query "lignin peroxidase" -spell -field DEFINITION

## plan: 

## check size of Cronn project, see if we have room on this computer to download all. 
##  if not, just grab a couple more to train methods
## get mRNA sequences for them all:

cutinase[All Fields] AND (fungi[filter] AND biomol_mrna[PROP])


## how can we do this with entrez?

## figure out how to query keywords and limit to fungi.

## get list of terms of interest. 

## can we take top, best hits?


## ugh. Another night and nothing to show for it. 

## Okay, we need to show the case for two ideas:

## 1 - there are fungal reads in these studies. 

## 2 - there are correlated, potentially ecologically meaningful 
##     changes in the fungal-origin and host RNA. 

## so - for #1, get . Set up the runinfo, do the following:

## download a sample
## blast it for the set of fungal indicators
## collect counts and run metadata
## delete r

## let's tool around in talapas for a bit:

## get an interactive session in talapas

srun --pty bash

## our storage is at:

cd /projects/xylaria/dthomas

## modules
module load blast
module load bbmap

## we need to get a list of all the projects/samples
## of Dougfir from SRA:

## there are three projects I'm interested in from the Cronn folks:
PRJNA263611
PRJNA188506
PRJNA243096
## and one project from the EPA:
PRJNA421903


## to get a list of runs/samples from this project

proj='PRJNA188506' 
wget 'http://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?save=efetch&rettype=runinfo&db=sra&term='$proj -O SraRunInfo2.csv

## okay, so once you have this, how do you get the files?

## I think we just want the first column:

bb=$(cut -d ',' -f 1  SraRunInfo.csv)

for i in ${bb}; do 
echo $i
done

## taking just the last one, can we download the sample using this value?

## can we get sra tools work locally on talapas?
wget https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-centos_linux64.tar.gz
ln -s /projects/xylaria/dthomas/sratoolkit/bin/prefetch /projects/xylaria/dthomas/prefetch
ln -s /projects/xylaria/dthomas/sratoolkit/bin/fastq-dump /projects/xylaria/dthomas/fastq-dump

## anyway, can get the raw reads from one SRA run ID?

./fastq-dump -Z $i > $i.fastq

## seems to be working...

## got to convert to fasta - best utility for this?
## I remember bbmap was good...

reformat.sh in=$i.fastq out=$i.fasta

## now to blast it for sequences of interest?

makeblastdb -in $i.fasta -parse_seqids -dbtype nucl &



## after getting the results, cleanup:
## rm $i.fastq 

## accidentally put the fastq at:
ls -l /projects/xylaria/dthomas/sratoolkit/bin/SRR669871.fastq

## we need some genes:

## how to build a fasta with what we need?


## ergosterol:

## we want everything after lanosterol for the 
## ergosterol pathway, I guess. Our genbank
## search terms:

## Ergosterol[Title] AND (fungi[filter] AND biomol_mrna[PROP])




## fungal chitan synthase

## this is my search:
## chitin synthase[Title] AND (fungi[filter] AND biomol_mrna[PROP])
## then I kept the top 195 records

## from home comp:
scp chitin_synthase_search_Genbank.fasta dthomas@talapas-ln1.uoregon.edu:/projects/xylaria/dthomas 

scp Ergosterol_search_Genbank.fasta dthomas@talapas-ln1.uoregon.edu:/projects/xylaria/dthomas 

## do a search with these?

blastn -db $i.fasta -query chitinase_search_Genbank.fasta -out $i.chs.out

## nada. is it because the queries are so long? Make a small one:

blastn -db $i.fasta -query test.fasta -out test.chs.out

## nope. Hmm. Well, this may in fact be a bad idea...

## anyway, let's take this to it's logical end. Tonight:
## 1 - compile full fasta of genes to look for, ergosterols and chitin synthases
## 2 - figure out a good blast output format to score hits, if any, quickly
## 3 - write script to loop through one project
## 4 - apply this to all 4 projects

## and talapas is down... fuck. This has been a difficult grant application
## to write...

## new plan - start on manuscript edits. Maybe the text for the grant

## hit the ground running on this weekend. By then I should have access to 
## either wm or uo cluster or both. 

## but I need to know that I'm not crazy here, that this is isn't a waste 
## of time. 

## let's use the lab computer till talapas is back online. 

## two projects - the above involves checking the entire read set for the 
## presence of a set of fungal-specific reads. The other half of my 
## preliminary data idea was to simply get an idea of how many reads are 
## thrown out from an RNAseq study wsimply due to not aligning to the 
## ref genome or transcriptome. 

## these are complimentary. One iteration of the loop should look this:

## download a run or sample
## align it 
## discard reads that align
## note amount of non-aligned reads
## blast the remaining sample with the selected sequences from ergosterol and chitin synthase

## get the Dougfir genome:

cd /home/daniel/Documents/genomes/Pseudotsuga

wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/001/517/045/GCA_001517045.1_DougFir1.0/GCA_001517045.1_DougFir1.0_genomic.fna.gz

tar -xzf GCA_001517045.1_DougFir1.0_genomic.fna.gz

## that's going to take forever. But what are we going to do with it when it's done?

## make an index file for  hisat:
hisatDir=/home/daniel/hisat2-2.1.0

$hisatDir/hisat2-build GCA_001517045.1_DougFir1.0_genomic.fna dougFirGen

##########################
#!usr/bin/env bash
hisat2-build GCA_001517045.1_DougFir1.0_genomic.fna dougFirGen

## get list of runs for the first project

proj='PRJNA188506' 
wget 'http://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?save=efetch&rettype=runinfo&db=sra&term='$proj -O SraRunInfo.csv

## make an array of run names:

bb=$(cut -d ',' -f 1  <(sed '1,1 d' SraRunInfo.csv ))

## get the fastq

for i in $bb; do
echo $i
    ## get fastq
    ./fastq-dump -Z $i > $i.fastq
    ## do the alignment
    sa=${i/\.fastq/\.sam}
    $hisatDir/hisat2 -x mllGen -U $i -S $sa 
    ## convert to BAM
    ba=${i/\.sam/\.bam}
    samtools view -bS $sa > $ba
    ## or convert to fasta?
    clean up 
    rm $sa
    rm $i
    ## we need to count lines here
done


$hisatDir/hisat2

## 

reformat.sh in=$i.fastq out=$i.fasta
makeblastdb -in $i.fasta -parse_seqids -dbtype nucl &


## setup scp for dougfir genome:

scp GCA_001517045.1_DougFir1.0_genomic.fna.gz dthomas@talapas-login.uoregon.edu:/projects/xylaria/dthomas

## get hisat

wget ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/downloads/hisat2-2.1.0-Linux_x86_64.zip

## can we get a job going to build the hisat index?

#!/bin/bash
#SBATCH --partition=short        ### Partition (like a queue in PBS)
#SBATCH --job-name=build_dougfir_index
#SBATCH --output=build.log         ### File in which to store job output
#SBATCH --error=build.err          ### File in which to store job error messages
#SBATCH --time=0-04:01:00       ### Wall clock time limit in Days-HH:MM:SS
#SBATCH --nodes=1               ### Number of nodes needed for the job
#SBATCH --ntasks-per-node=1     ### Number of tasks to be launched per Node

hisat2-2.1.0/hisat2-build GCA_001517045.1_DougFir1.0_genomic.fna dougfir

scp hisatbuild.sh dthomas@talapas-login.uoregon.edu:/projects/xylaria/dthomas

## we're going to do two large loops, (1) through all the samples in a project, 
## for (2) all projects 

## so let's run through one sample from one project:


project='PRJNA188506' 
wget 'http://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?save=efetch&rettype=runinfo&db=sra&term='$project -O $project".csv"

module load samtools


for i in ${bb[@]}; do
echo $i
done

######### 
#!/bin/bash
#SBATCH --partition=short        ### Partition (like a queue in PBS)
#SBATCH --job-name=build_dougfir_index
#SBATCH --output=build.log         ### File in which to store job output
#SBATCH --error=build.err          ### File in which to store job error messages
#SBATCH --time=0-23:00:00       ### Wall clock time limit in Days-HH:MM:SS
#SBATCH --nodes=1               ### Number of nodes needed for the job
#SBATCH --ntasks-per-node=1     ### Number of tasks to be launched per Node
#SBATCH --memory=64G

module load samtools
module load bbmap
cd /projects/xylaria/dthomas

project='PRJNA188506' 
wget 'http://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?save=efetch&rettype=runinfo&db=sra&term='$project -O $project".csv"
bb=($(cut -d ',' -f 1  <(sed '1,1 d' $project".csv" )))

for i in ${bb[@]}; do
    echo $i
    ./fastq-dump -Z $i > $i.fastq
    hisat2-2.1.0/hisat2 -x dougfir -U $i.fastq -S $i.sam 
    rm $i.fastq
    samtools view -f 4 -b $i.sam > $i"_notMapped.bam"
    rm $i.sam
    ## save this for later, keeps failing:
    #reformat.sh in=$i"_notMapped.bam" out=$i"_notMapped.fastq"
done

## cleanup:
#find . -type f -newer notMatch.sh -exec "rm" {} \;

############################################################

aa=('PRJNA263611' 'PRJNA188506' 'PRJNA243096' 'PRJNA421903')
for j in ${aa[@]}; do
echo $j
done

## now what? 

## we should take our fastq file of unmatched reads, and blast 
## them against known fungal genes. 

## which genes?

## tonight - get downloads needed for pollution experiment, and one or two budbreak

## but let's get the pollution downloads started. We don't need them all, just a few,
## so make a list on genbank and start the pipeline.

## then write a script to get the bam -> fastq conversions? See if we have 
## room...

bb=($(cut -d ',' -f 1  <(sed '1,1 d' dougFirPollutionRuns.txt)))

head -n 2 dougFirPollutionRuns.txt 

head -n 2 dougFirPollutionRuns.txt | cut  -f 5

echo $(sed '1,1 r' dougFirPollutionRuns.txt)

bb=($(cut -f 5  <(sed '1,1 d' dougFirPollutionRuns.txt)))
for i in ${bb[@]}; do 
echo $i
done

## tomorrow, convert files to fastq. 
## look for an automated way to blast them 
## against fungi (check paper)


####################

## let's try one manual blast, see how long this takes:

## on talapas, convert to fastq:

cd /projects/xylaria/dthomas/budbreak

module load samtools
module load bbmap

reformat.sh in=SRR669812_notMapped.bam out=SRR669812_notMapped.fa


scp dthomas@talapas-login.uoregon.edu:/projects/xylaria/dthomas/budbreak/SRR669812_notMapped.fa .

## for our fungal RNA database from genbank:
"Fungi"[Organism] AND (fungi[filter] AND biomol_mrna[PROP] AND ddbj_embl_genbank[filter])

## gives us ~200,000 sequences to make a blastn database...

## for our bacterial database, autogenerate from the genbank site:

(("Bacteria"[Organism] OR "Bacteria Latreille et al. 1825"[Organism]) OR ("Bacteria"[Organism] OR "Bacteria Latreille et al. 1825"[Organism] OR bacteria[All Fields])) AND (biomol_mrna[PROP] AND ddbj_embl_genbank[filter]) AND (bacteria[filter] AND biomol_mrna[PROP])

## for archea, let's wait...

## get our fungal and bacterial RNA fastas onto talapas:

scp fungal_rna.txt dthomas@talapas-login.uoregon.edu:/projects/xylaria/dthomas/

scp bacterial_rna.txt dthomas@talapas-login.uoregon.edu:/projects/xylaria/dthomas/

## make blast databases

## interactive
srun --pty bash

module load blast
module load samtools
module load bbmap

## fungal db
cd /projects/xylaria/dthomas/fungal_RNA_blastdb
makeblastdb -in fungal_rna.fa -dbtype nucl 
## so our fungal database is at:
fdb=/projects/xylaria/dthomas/fungal_RNA_blastdb/fungal_rna.fa

## bacterial db
cd /projects/xylaria/dthomas/bacterial_RNA_blastdb
makeblastdb -in bacterial_rna.fa -dbtype nucl 
## our bacterial database is at:
bdb=/projects/xylaria/dthomas/bacterial_RNA_blastdb/bacterial_rna.fa

## for some reason, both of these have ~10,000 sequences in them. 
## I thought there were more in the request...

## how does converting one of our bams to fasta, then blasting work?

cd budbreak/

i="SRR669812_notMapped.bam"

echo ${i/.bam/.fa}

reformat.sh in=$i out=${i/.bam/.fa}

## sure how this works, since I can't edit the path for blast...
## put the db and query in the same folder?

cp /projects/xylaria/dthomas/fungal_RNA_blastdb/* ./


## run blast:


time blastn -db $fdb -query ${i/.bam/.fa} -outfmt "6 pident qlen" -out test.out
time blastn -db $bdb -query ${i/.bam/.fa} -outfmt "6 pident qlen" -out test.out

## that will for now, we can count the lines to get a raw estimate of fungal and bacterial 
## reads. 

echo ${i/.bam/.fa}

echo ${i/.bam/_fungi.out}

echo ${i/.bam/_bact.out}

## how do we script this? 


## lookForMicrobes.sh
##################################
#!/bin/bash
#SBATCH --job-name=lookForMicrobes
#SBATCH --output=lookForMicrobes.log         ### File in which to store job output
#SBATCH --error=lookForMicrobes.err          ### File in which to store job error messages
#SBATCH --time=1-00:00:00       ### Wall clock time limit in Days-HH:MM:SS
#SBATCH --nodes=1               ### Number of nodes needed for the job
#SBATCH --ntasks-per-node=1     ### Number of tasks to be launched per Node
#SBATCH --mem=64G

module load blast
module load samtools
module load bbmap

fdb=/projects/xylaria/dthomas/fungal_RNA_blastdb/fungal_rna.fa
bdb=/projects/xylaria/dthomas/bacterial_RNA_blastdb/bacterial_rna.fa

cd /projects/xylaria/dthomas/pollution
#cd /projects/xylaria/dthomas/budbreak

for i in *; do
    echo $i
    reformat.sh in=$i out=${i/.bam/.fa}
    time blastn -db $fdb -query ${i/.bam/.fa} -max_target_seqs 1 -outfmt "6 qseqid pident qlen" -out ${i/.bam/_fungi.out}
    time blastn -db $bdb -query ${i/.bam/.fa} -max_target_seqs 1 -outfmt "6 qseqid pident qlen" -out ${i/.bam/_bact.out}
    rm ${i/.bam/.fa}
done


## side note, found this for remote blasts. Still works?
## $ blastn –db nt –query nt.u00001 –out test.out -remote

## get our blast results

## seems  like there may be some duplicates, not sure why, I asked for only top hits:

cut SRR669789_notMapped_fungi.out -f 1 | uniq -d | wc -l 
## 19, out of...
wc -l SRR669789_notMapped_fungi.out
## 3107


## okay, what do we need? 

GTTTGGGGTGAGGGGCATGCTGTGGGAGATATTCCTGGAGTTCGGTTTAAGGTTGTTAAGGTGTCTGGTGTATCCTTGTTAGCTCTCTTCAAGGAGAAAAA

## 1) We need the raw reads total for each run. 

## this can be gotten from the project information on SRA. Is there an easy way to do this?

## for the budbreak project, these are our runs: 
## budbreak only has forward reads, 
## so "spots" = "reads"
## budbreak read length is 101 bp

cd /projects/xylaria/dthomas/budbreak

touch lcBudbreakFungi.txt
touch lcBudbreakBacteria.txt
for i in *fungi.out; do
echo $(wc -l $i) >> lcBudbreakFungi.txt
done
for i in *bact*.out; do
echo $(wc -l $i) >> lcBudbreakBacteria.txt
done

## check for repeats
for i in *fungi.out; do
echo $(cut -f 1 $i | uniq -d) | wc -w
done

for i in *bact.out; do
echo $(cut -f 1 $i | uniq -d) | wc -w
done

## budbreak_readTotals.csv
sample, total_reads, fungal_reads, fungal_repeats, bacterial_reads, bacterial_repeats
SRR669789, 29989349, 3107, 19, 43,  0
SRR669790, 31236333, 726,  5,  24,  0
SRR669791, 48157375, 5903, 20, 115, 0
SRR669792, 21466916, 2274, 14, 20,  0
SRR669793, 32993821, 4659, 14, 41,  0
SRR669794, 32091057, 563,  7,  33,  0
SRR669795, 27829059, 4724, 17, 87,  0
SRR669806, 52716047, 5931, 27, 132, 0
SRR669807, 32138767, 1002, 14, 55,  0
SRR669812, 34546541, 2762, 9,  50,  0
SRR669813, 35358876, 758,  18, 50,  0
SRR669814, 37651083, 5282, 16, 86,  0
SRR669818, 22435789, 1005, 15, 48,  0

## and for the pollutions study, there are forward and reverse reads,
## but doesn't really matter, seems like the fungal reads should be
## forward and reverse, so the ratio should be ~ the same. 
## pollution read length is 51 bp

cd /projects/xylaria/dthomas/pollution

touch lcPollutionFungi.txt
touch lcPollutionBacteria.txt

for i in *fungi.out; do
echo $(wc -l $i) >> lcPollutionFungi.txt
done
for i in *bact*.out; do
echo $(wc -l $i) >> lcPollutionBacteria.txt
done

## check for repeats
for i in *fungi.out; do
echo $(cut -f 1 $i | uniq -d) | wc -w
done

for i in *bact.out; do
echo $(cut -f 1 $i | uniq -d) | wc -w
done

## pollution_readTotals.csv
sample, total_reads, fungal_reads, fungal_repeats, bacterial_reads, bacterial_repeats
SRR6365617, 12000000, 32376,  2175, 230, 1
SRR6365619, 12000000, 19151,  1271, 84,  0
SRR6365620, 12000000, 18865,  1342, 100, 0
SRR6365622, 12000000, 28046,  1944, 142, 0
SRR6365623, 10180376, 22592,  1489, 109, 0
SRR6365646, 12000000, 60083,  780,  101, 0
SRR6365648, 12000000, 73810,  2312, 119, 3
SRR6365650, 12000000, 60272,  3192, 70,  0
SRR6365652, 12000000, 117135, 3233, 25,  0
SRR6365653, 12000000, 113369, 2999, 54,  1
SRR6365655, 10799783, 30056,  554,  87,  0

## okay, so now what?

## read these in as pandas, cleanup, plot






















